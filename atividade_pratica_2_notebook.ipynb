{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atividade Prática II - Treinamento e Validação de Modelos de RL\n",
    "\n",
    "**Aluno:** LEANDRO STARKE\n",
    "\n",
    "**Disciplina:** Reinforcement Learning - Turma I\n",
    "\n",
    "**Data:** 03/07/2021\n",
    "\n",
    "\n",
    "\n",
    "Neste trabalho vamos aplicar `Gym`, `Stable-Baselines3` e `RL Baselines Zoo` para lidar com o treinamento e validação de problemas de aprendizado por reforço. Sua tarefa é:\n",
    "\n",
    "1. Selecionar um cenário da biblioteca `Gym` de sua preferência, desde que este cenário também seja contemplado pelos modelos disponibilizados na `rl baselines zoo`;\n",
    "2. Selecionar três algoritmos das biblioteca `Stable-baselines3` para resolver esse problema. Pesquise na documentação da biblioteca quais são os algoritmos mais adequados para o ambiente escolhido e justifique a sua escolha. \n",
    "3. Realize o treinamento de cada um dos três modelos ---você pode ajustar os parâmetros do modelos, se achar necessário--- e salve os modelos em disco.\n",
    "4. De posse dos modelos treinados e salvos, carregue-os e avalie-os por 10 episódios. Apresente os resultados médios e gere a curva de recompensa acumulada disponibilizada pelo `TensorBoard`.\n",
    "5. Compare os resultados dos modelos treinados com os resultados obtidos por modelo(s) existentes no `RL Baselines Zoo` para o cenário escolhido.\n",
    "6. Gere um vídeo do melhor modelo que você treinou e do modelo escolhido na `RL Baselines Zoo`. Verifique a documentação de cada biblioteca sobre a criação do vídeo e visualização em Notebooks.\n",
    "\n",
    "\n",
    "\n",
    "* **Data de entrega:** 16/07/2021\n",
    "* **Local de envio:** AVA.\n",
    "* **Tipo de documento:** Notebook (`.ipynb`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym\n",
    "!git clone https://github.com/openai/gym\n",
    "%cd gym\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Selecionar um cenário da biblioteca `Gym` de sua preferência, desde que este cenário também seja contemplado pelos modelos disponibilizados na `rl baselines zoo`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando ambiente\n",
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Selecionar três algoritmos das biblioteca `Stable-baselines3` para resolver esse problema. Pesquise na documentação da biblioteca quais são os algoritmos mais adequados para o ambiente escolhido e justifique a sua escolha.\n",
    "\n",
    "**Escolhi os algoritmos DQN, A2C e PPO tomando como base a tabela deste [link](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html).  \n",
    "Pelo motivo que o LunarLander ter quatro ações discretas selecionei os algorimos com a coluna \"discrete\" marcada.\n",
    "Também fiz uma pesquisa no google procurando melhores algoritmos para resolver o Lunar Lander, em todos os resultados que encontrei foram usados o algoritmos DQN**\n",
    "3. Realize o treinamento de cada um dos três modelos ---você pode ajustar os parâmetros do modelos, se achar necessário--- e salve os modelos em disco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciando modelo utilizando o algoritmo DQN\n",
    "model = DQN('MlpPolicy', env, verbose=1, tensorboard_log=\"./tensorboard/\")\n",
    "\n",
    "# Treinando o modelo\n",
    "model.learn(total_timesteps=int(2e5))\n",
    "\n",
    "# Salvando o modelo\n",
    "model.save(\"dqn_lunar\")\n",
    "\n",
    "###################\n",
    "\n",
    "# Iniciando modelo utilizando o algoritmo A2C\n",
    "model = A2C('MlpPolicy', env, verbose=1, tensorboard_log=\"./tensorboard/\")\n",
    "\n",
    "# Treinando o modelo\n",
    "model.learn(total_timesteps=int(2e5))\n",
    "\n",
    "# Salvando o modelo\n",
    "model.save(\"a2c_lunar\")\n",
    "\n",
    "###################\n",
    "\n",
    "# Iniciando modelo utilizando o algoritmo PPO\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=\"./tensorboard/\")\n",
    "\n",
    "# Treinando o modelo\n",
    "model.learn(total_timesteps=int(2e5))\n",
    "\n",
    "# Salvando o modelo\n",
    "model.save(\"ppo_lunar\")\n",
    "\n",
    "#del model # deletando modelo para garantir que ele não exista no momento o load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. De posse dos modelos treinados e salvos, carregue-os e avalie-os por 10 episódios. Apresente os resultados médios e gere a curva de recompensa acumulada disponibilizada pelo `TensorBoard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando modelo salvo\n",
    "model = DQN.load(\"dqn_lunar\", env=env, tensorboard_log=\"./tensorboard/\")\n",
    "\n",
    "# Avaliando o modelo por 10 episodios\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "# Testando modelo treinado\n",
    "#obs = env.reset()\n",
    "#for i in range(5000):\n",
    "#    action, _states = model.predict(obs, deterministic=True)\n",
    "#    obs, rewards, dones, info = env.step(action)\n",
    "#    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando modelo salvo\n",
    "model = A2C.load(\"a2c_lunar\", env=env, tensorboard_log=\"./tensorboard/\")\n",
    "\n",
    "# Avaliando o modelo por 10 episodios\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "# Testando modelo treinado\n",
    "#obs = env.reset()\n",
    "#for i in range(5000):\n",
    "#    action, _states = model.predict(obs, deterministic=True)\n",
    "#    obs, rewards, dones, info = env.step(action)\n",
    "#    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando modelo salvo\n",
    "model = PPO.load(\"ppo_lunar\", env=env, tensorboard_log=\"./tensorboard/\")\n",
    "\n",
    "# Avaliando o modelo por 10 episodios\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "# Testando modelo treinado\n",
    "#obs = env.reset()\n",
    "#for i in range(5000):\n",
    "#    action, _states = model.predict(obs, deterministic=True)\n",
    "#    obs, rewards, dones, info = env.step(action)\n",
    "#    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando vídeos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Compare os resultados dos modelos treinados com os resultados obtidos por modelo(s) existentes no `RL Baselines Zoo` para o cenário escolhido.\n",
    "6. Gere um vídeo do melhor modelo que você treinou e do modelo escolhido na `RL Baselines Zoo`. Verifique a documentação de cada biblioteca sobre a criação do vídeo e visualização em Notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
    "os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "\n",
    "def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n",
    "  \"\"\"\n",
    "  :param env_id: (str)\n",
    "  :param model: (RL model)\n",
    "  :param video_length: (int)\n",
    "  :param prefix: (str)\n",
    "  :param video_folder: (str)\n",
    "  \"\"\"\n",
    "  eval_env = DummyVecEnv([lambda: gym.make('LunarLander-v2')])\n",
    "  # Start the video at step=0 and record 500 steps\n",
    "  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n",
    "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
    "                              name_prefix=prefix)\n",
    "\n",
    "  obs = eval_env.reset()\n",
    "  for _ in range(video_length):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, _, _, _ = eval_env.step(action)\n",
    "\n",
    "  # Close the video recorder\n",
    "  eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "\n",
    "model = DQN.load(\"dqn_lunar\", env=env, tensorboard_log=\"./tensorboard/\")\n",
    "record_video('LunarLander-v2', model, video_length=1000, prefix='dqn_lunar')\n",
    "\n",
    "model = A2C.load(\"a2c_lunar\", env=env, tensorboard_log=\"./tensorboard/\")\n",
    "record_video('LunarLander-v2', model, video_length=1000, prefix='a2c_lunar')\n",
    "\n",
    "model = PPO.load(\"ppo_lunar\", env=env, tensorboard_log=\"./tensorboard/\")\n",
    "record_video('LunarLander-v2', model, video_length=1000, prefix='ppo_lunar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir ./tensorboard/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparativos:\n",
    "\n",
    "***Modelos treinados no trabalho***  \n",
    "Algoritmo DQN: mean_reward = -125.69 std_reward = 17.48  \n",
    "Algoritmo A2C: mean_reward =  58.86 std_reward = 107.79  \n",
    "Algoritmo PPO: mean_reward = 98.96 std_reward 99.67  \n",
    "\n",
    "***Modelo treinados prontos RL Baselines3 Zoo***  \n",
    "Algoritmo DQN: mean_reward = 139.37 std_reward = 63.49    \n",
    "Algoritmo A2C: mean_reward = 129.80 std_reward = 74.26  \n",
    "Algoritmo PPO: mean_reward = 250.47 std_reward = 16.77  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
